{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pUb8KpyN5QSp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "76b51ccc-06cd-4df1-8555-32cd0be7dd38"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nTitle:\\nAuthor:\\nDate\\nDiscription: This notbook contains basics for the LLM Project.\\nTo_deploy: Streamlit\\n--\\n\\nReferences:\\nStep by Step  vedio:\\nhttps://www.freecodecamp.org/news/how-to-build-a-large-language-model-from-scratch-using-python/ To Follow this video\\nhttps://github.com/ajitsingh98/LLMs-From-Scratch-In-PyTorch\\n\\nBook / Reading:\\nhttps://github.com/ManalAbumelhaa/LLM-LLMs-from-scratch\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "'''\n",
        "Title: Birgam LLM Model\n",
        "Author: Siddharth Sharma\n",
        "Date: Aug, 24,2024 -- PST\n",
        "Discription: This notbook contains basics for the LLM Project. - A Bigram Language Model.\n",
        "--\n",
        "To_deploy: Streamlit\n",
        "--\n",
        "\n",
        "References:\n",
        "Step by Step  vedio:\n",
        "https://www.freecodecamp.org/news/how-to-build-a-large-language-model-from-scratch-using-python/ To Follow this video\n",
        "https://github.com/ajitsingh98/LLMs-From-Scratch-In-PyTorch\n",
        "\n",
        "Book / Reading:\n",
        "https://github.com/ManalAbumelhaa/LLM-LLMs-from-scratch\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iinstall the compression algo\n",
        "!pip install pylzma"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZi7bvx7jv7N",
        "outputId": "786423f5-e188-48b9-cf5c-68c0f84ea3e5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pylzma\n",
            "  Downloading pylzma-0.5.0.tar.gz (4.2 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/4.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m3.9/4.2 MB\u001b[0m \u001b[31m115.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m108.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pylzma\n",
            "  Building wheel for pylzma (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pylzma: filename=pylzma-0.5.0-cp310-cp310-linux_x86_64.whl size=222315 sha256=f798af9174b28a68c707fe2c49b03281ea851e6d121d8aeb05614d95cb1b8569\n",
            "  Stored in directory: /root/.cache/pip/wheels/74/c9/02/91112815e838f544c1d46fda071241e454694579d022751d2b\n",
            "Successfully built pylzma\n",
            "Installing collected packages: pylzma\n",
            "Successfully installed pylzma-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pylzma\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time"
      ],
      "metadata": {
        "id": "vmsOISyudTgS"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pf1D7FqAtotT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount ('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDNSf0zcms9Z",
        "outputId": "e44a8d14-b0f1-4177-c9b1-ec67a5bad0d4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open (\"/content/drive/MyDrive/Colab Notebooks/From Scratch/Data Sets/wizard_of_oz.txt\" , 'r' , encoding = 'utf-8') as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "61DmnVdQma9P"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:300]) # if we do simply text[:300] it will not be visually appealing as it will also have the newline characters."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RKHt1oCnCh4",
        "outputId": "6431947f-4485-4d32-ccea-f1408f34fb02"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Title: The Wonderful Wizard of Oz\n",
            "\n",
            "Author: L. Frank Baum\n",
            "\n",
            "Release date: February 1, 1993 [eBook #55]\n",
            "                Most recently updated: March 30, 2021\n",
            "\n",
            "Language: English\n",
            "\n",
            "\n",
            "\n",
            "*** START OF THE PROJECT GUTENBERG EBOOK THE WONDERFUL WIZARD OF OZ ***\n",
            "\n",
            "[Illustration]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "The Wonderful Wizard of Oz\n",
            "\n",
            "by\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cB811pbnJAr",
        "outputId": "1b78d316-85c7-400c-f082-6ee9b8e694c5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "208087"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now need to build a tokenizer, wherein we will see how many characters do the whole text contains and this tokenizer will contain of a encoder and a decoder, each character will be assigned a number or a  (HASH)\n",
        "\n",
        "\n",
        "This is a character level tokenizer,\n",
        "\n",
        "there are other - sub word level, sentence level and word level tokenizer's"
      ],
      "metadata": {
        "id": "h2zXq91Bn3kl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(set(text))\n",
        "print(chars) # this contains all the different characters present in the text.\n",
        "print(len(chars))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPnoBj7RnUtR",
        "outputId": "2b3f0fe7-0cf2-4dd9-8ba6-c532ca435866"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', ' ', '!', '#', '&', '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '5', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '—', '‘', '’', '“', '”']\n",
            "79\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "string_to_int = {ch:i for i,ch in enumerate(chars)} # in dict format\n",
        "int_to_string = {i:ch for i,ch in enumerate(chars)}\n",
        "\n",
        "# print(string_to_int)\n",
        "# print(int_to_string)\n",
        "\n",
        "encode = lambda s:[string_to_int[c] for c in s] # keeping it simple here, but better encoding  will make the custom code more secure...\n",
        "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
        "\n",
        "# sample to see if the intended code working or not\n",
        "print(encode(\"testing\"))\n",
        "print(encode(\"Testing\"))\n",
        "\n",
        "print(decode(encode(\"Testing\")))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wcyh0gMXpKdW",
        "outputId": "e1c1f244-f208-4a93-8ca9-b72dc279a775"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[67, 52, 66, 67, 56, 61, 54]\n",
            "[39, 52, 66, 67, 56, 61, 54]\n",
            "Testing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "torch.long - we are going to create a tensor and we will have the tensor conaitn super long integers\n",
        "\n",
        "we will be storign our data in tensors now."
      ],
      "metadata": {
        "id": "ysJZH8rBrelx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encode(text), dtype = torch.long)\n",
        "print(data[:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLUE0l_rreLR",
        "outputId": "e63018e4-1581-43b2-fc6a-d08c927fdb7c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0, 39, 56, 67, 59, 52, 17,  1, 39, 55, 52,  1, 42, 62, 61, 51, 52, 65,\n",
            "        53, 68, 59,  1, 42, 56, 73, 48, 65, 51,  1, 62, 53,  1, 34, 73,  0,  0,\n",
            "        20, 68, 67, 55, 62, 65, 17,  1, 31, 10,  1, 25, 65, 48])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now once the data is defined in tensor's we will be splitting the data into training and testing split. the text corpus now. (70/30) split since the data corpus is small.\n",
        "\n",
        "Link of the chat: https://chatgpt.com/share/6d4cb86a-e766-4796-8c74-63381191184d\n",
        "\n",
        "***Bigram Models***\n",
        "\n",
        " bigram model is a type of statistical language model used in natural language processing (NLP) that predicts the probability of a word given the previous word in a sequence. In simpler terms, it considers pairs of consecutive words (known as \"bigrams\") to understand the likelihood of a word following another in a sentence.\n",
        "\n",
        "How it works:\n",
        "Sequence of Words: The bigram model breaks down a text into pairs of consecutive words.\n",
        "For example, in the sentence \"I am learning NLP,\" the bigrams would be (\"I\", \"am\"), (\"am\", \"learning\"), and (\"learning\", \"NLP\").\n",
        "Probability Calculation: The model calculates the probability of a word given the previous word,\n",
        "\n",
        "\n",
        "appears as the first word in any bigram in the text.\n",
        "Application: This model is used to predict the next word in a sequence, generate text, or assess the likelihood of a given sequence of words. It's relatively simple but provides a foundational understanding of word dependencies in text.\n",
        "Limitations:\n",
        "Context: The bigram model only considers the previous word, so it doesn't capture long-range dependencies or context beyond one word back.\n",
        "Data Sparsity: If certain bigrams are rare or unseen in the training data, the model might struggle to predict them accurately.\n",
        "Overall, while the bigram model is a fundamental and straightforward approach, it is often extended to more complex models like trigrams (which consider three-word sequences) or even more sophisticated models like neural networks that capture broader context.\n",
        "\n"
      ],
      "metadata": {
        "id": "aXKqew3ZsB0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a test train split:\n",
        "# 70 percent training\n",
        "threshold= 0.7*len(data)\n",
        "\n",
        "train_text = data[:int(threshold)]\n",
        "test_text = data[int(threshold):]\n"
      ],
      "metadata": {
        "id": "x51iqEipqFDc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 6\n",
        "\n",
        "x = train_text[:block_size]\n",
        "y = train_text[1:block_size+1] # increamenting the character by 1, to see whether if it changes the proba of the otucomes.\n",
        "\n",
        "for block_word in range (block_size):\n",
        "  context = x[:block_word +1] # the context will keep on increasing, as the for loops progress, at the end of the loop the context will be the same size of 8 block words (here) (or the prompt data])\n",
        "  target = y[block_word] #\n",
        "  print(f\"when input is {context} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mvwlFcZx9WI",
        "outputId": "c4ddcb47-4501-455e-efd1-b1a71f33a707"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([0]) the target: 39\n",
            "when input is tensor([ 0, 39]) the target: 56\n",
            "when input is tensor([ 0, 39, 56]) the target: 67\n",
            "when input is tensor([ 0, 39, 56, 67]) the target: 59\n",
            "when input is tensor([ 0, 39, 56, 67, 59]) the target: 52\n",
            "when input is tensor([ 0, 39, 56, 67, 59, 52]) the target: 17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the block size is defined as 8 and the increment as 1, we  want to scale it up and hence we are wanting to process this in batches: (easist wyay to scale up and perform deep learningnlp tasks)\n",
        "\n",
        "I'd be using the basic GPU T4 - Google collab easily available for a bit of faster processing, else CPU"
      ],
      "metadata": {
        "id": "GQ2upciz1UNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A simple preformance metric.\n",
        "start_time = time.time()\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time # if we want to see the process\n",
        "print(f\"Elapsed time: {elapsed_time} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qi7T5dKQ0JOu",
        "outputId": "3d0700f9-c109-4db5-bb24-c34dccef35aa"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed time: 2.86102294921875e-05 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_1 = torch.tensor([1.,2.,3.])\n",
        "\n",
        "# get probas / Apply softmax\n",
        "softmax_output = F.softmax(tensor_1, dim=0)\n",
        "\n",
        "# NOTE: Why dim=0?\n",
        "# In this context, dim=0 is used because tensor_1 is a 1-dimensional tensor.\n",
        "# If it were a multi-dimensional tensor, dim would control along which dimension the softmax operation is applied.\n",
        "# Since there's only one dimension here, dim=0 is appropriate.\n",
        "# If you were working with a 2D tensor (e.g., a matrix), specifying dim=0 would apply the softmax across columns,\n",
        "# while dim = 1 would apply it across rows.\n",
        "\n",
        "\n",
        "print(softmax_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLZnGv_h228o",
        "outputId": "2d579620-8142-49d4-e4ea-796eccc62dc6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.0900, 0.2447, 0.6652])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Todo: Learn a bit more about the nn.embeddings layering nd how this embedding is done."
      ],
      "metadata": {
        "id": "5nwCsQdkzth6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an embedding layer\n",
        "vocab_size = 10000\n",
        "embedding_dim = 100\n",
        "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "# Create some input indices\n",
        "input_indices = torch.LongTensor([1,5, 3, 2])\n",
        "# Apply the embedding layer\n",
        "\n",
        "embedded_output = embedding(input_indices)\n",
        "# The output will be a tensor of shape (4, 100), where 4 is the number of input\n",
        "# and 100 is the dimensionality of the embedding vectors\n",
        "\n",
        "print(embedded_output.shape)\n",
        "embedded_output_with_nan = torch.where(embedded_output > 1, embedded_output, torch.tensor(np.nan))\n",
        "print(embedded_output_with_nan)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WV2onn914hay",
        "outputId": "59091bed-d7c9-4e33-df3f-72ffc4e88fc0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 100])\n",
            "tensor([[   nan,    nan,    nan,    nan,    nan,    nan, 1.2166, 1.0423,    nan,\n",
            "            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
            "            nan, 2.8008,    nan,    nan,    nan,    nan,    nan,    nan, 1.6144,\n",
            "            nan, 1.4763,    nan,    nan, 1.3010,    nan,    nan,    nan, 1.0067,\n",
            "            nan,    nan,    nan,    nan,    nan,    nan, 1.0816,    nan,    nan,\n",
            "            nan, 1.3635, 1.2689, 1.5447,    nan,    nan,    nan, 1.3808,    nan,\n",
            "            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 2.4802,\n",
            "            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
            "            nan,    nan,    nan,    nan, 1.6646,    nan, 2.3350,    nan, 3.1220,\n",
            "            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 1.0483,\n",
            "            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 1.6848,\n",
            "            nan],\n",
            "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan, 1.3438,    nan,\n",
            "         1.5148,    nan,    nan,    nan,    nan,    nan,    nan, 1.7610, 2.0384,\n",
            "            nan,    nan,    nan,    nan,    nan, 1.5163,    nan,    nan,    nan,\n",
            "            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
            "            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
            "         1.2761,    nan,    nan,    nan,    nan,    nan,    nan, 1.8246,    nan,\n",
            "         1.0730,    nan,    nan,    nan,    nan, 1.9351,    nan,    nan, 1.1225,\n",
            "            nan,    nan,    nan, 1.1285,    nan,    nan, 1.0194,    nan,    nan,\n",
            "            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
            "         1.2781,    nan, 1.3335,    nan,    nan,    nan,    nan,    nan,    nan,\n",
            "            nan,    nan, 1.6114,    nan,    nan,    nan,    nan,    nan,    nan,\n",
            "            nan],\n",
            "        [   nan,    nan, 1.2826,    nan, 1.7865,    nan,    nan,    nan,    nan,\n",
            "            nan,    nan, 1.1643,    nan,    nan,    nan,    nan,    nan,    nan,\n",
            "            nan,    nan,    nan,    nan,    nan, 1.0800,    nan,    nan,    nan,\n",
            "            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 1.5347,\n",
            "            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
            "            nan,    nan, 2.6770,    nan, 1.3244,    nan,    nan,    nan, 2.1299,\n",
            "         1.4544,    nan,    nan,    nan,    nan, 1.0537,    nan,    nan, 1.3296,\n",
            "         1.7635,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 1.7382,\n",
            "            nan,    nan,    nan,    nan,    nan, 1.0917,    nan,    nan,    nan,\n",
            "            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
            "            nan,    nan,    nan,    nan,    nan,    nan, 1.5018,    nan,    nan,\n",
            "            nan],\n",
            "        [   nan,    nan,    nan, 1.6965,    nan, 1.2683,    nan,    nan,    nan,\n",
            "            nan,    nan,    nan,    nan,    nan,    nan, 2.2175,    nan,    nan,\n",
            "            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
            "            nan,    nan,    nan, 1.9443,    nan,    nan, 1.0033,    nan,    nan,\n",
            "            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
            "            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
            "            nan,    nan,    nan, 1.5665,    nan,    nan,    nan, 1.8143,    nan,\n",
            "            nan,    nan,    nan,    nan,    nan,    nan,    nan, 1.3812,    nan,\n",
            "         1.8042,    nan,    nan, 1.0158,    nan,    nan,    nan,    nan,    nan,\n",
            "            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
            "            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
            "            nan]], grad_fn=<WhereBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matmul"
      ],
      "metadata": {
        "id": "Ht_L4pQr2bZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# a = torch.tensor([[1,2],[3,4],[5,6]])\n",
        "# b = torch.tensor([[2,8,4],[10,11,12]])\n",
        "# print(a @ b)\n",
        "# @ or matmul is the same {kinda}, but in pytorch you can not multiply int and floats."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPD3HNOwz06x",
        "outputId": "13f351d5-0993-44fb-8500-809aa141d56b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 22,  30,  28],\n",
            "        [ 46,  68,  60],\n",
            "        [ 70, 106,  92]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now make a bigram model, and we will be keeping a size of 8 for the block. this is a character level bigram model and here the increament of the block \"characters\" will be by 1 (block size remaining the same)"
      ],
      "metadata": {
        "id": "4uqvJIoSx_e3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# making different env / Choosing Cuda for batch processing:\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' # just checks if gpu is available or not (here T4)\n",
        "print(device)\n",
        "\n",
        "# Constants:\n",
        "# hyper parameters\n",
        "block_size = 8\n",
        "batch_size = 4\n"
      ],
      "metadata": {
        "id": "UxHmYmen197Z"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch code:\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_text if split == 'train' else test_text\n",
        "  i_x = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  # print(len(data) - block_size, (batch_size,))\n",
        "  # print(len(data)) # why ?\n",
        "  print(i_x)\n",
        "  # print(i_x.shape)\n",
        "  x = torch.stack([data[i:i+block_size] for i in i_x])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in i_x])\n",
        "  x,y = x.to(device), y.to(device) # Cuda here -  to process the data in batches.\n",
        "  # in CPU the processing would be in sequence whic will take a lot of time and the\n",
        "  # context windows might also suffer, in better / advanced models.\n",
        "  return x,y\n",
        "\n"
      ],
      "metadata": {
        "id": "x-LxM4V-4RN0"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # print(train_text.shape)\n",
        "  x,y = get_batch('train')\n",
        "  print('inputs:')\n",
        "  print(x)\n",
        "  print('targets:')\n",
        "  print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVZUWzS64W1K",
        "outputId": "3f9b9879-0e2e-4796-b05a-28ea623fb3ad"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "145652 (4,)\n",
            "145660\n",
            "tensor([  2659,  59082, 116018, 104520])\n",
            "torch.Size([4])\n",
            "inputs:\n",
            "tensor([[70, 56, 67, 55,  1, 40, 61, 50],\n",
            "        [55,  1, 48, 54, 48, 56, 61, 10],\n",
            "        [51,  1, 48,  1, 61, 52, 50, 58],\n",
            "        [10,  0,  0, 77, 38, 62,  1, 28]], device='cuda:0')\n",
            "targets:\n",
            "tensor([[56, 67, 55,  1, 40, 61, 50, 59],\n",
            "        [ 1, 48, 54, 48, 56, 61, 10,  0],\n",
            "        [ 1, 48,  1, 61, 52, 50, 58,  8],\n",
            "        [ 0,  0, 77, 38, 62,  1, 28,  1]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "  # the table is needed for the probas of the different characters w.r.t\n",
        "  # different characters (proba of a coming after the character c, and so on.)\n",
        "\n",
        "  def __init__(self,vocab_size):\n",
        "    super().init_()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, indices, targets = None):\n",
        "    logits = self.token_embedding_table(indices)\n",
        "    Batch,Time, Channel = logits.shape\n",
        "    logits = logits.view(Batch*Time, Channel) # logits should have the same time and the batch\n",
        "    # The view simply reshapes the output of BTC as the f.cross entropy hasa pecular way of taking in data.\n",
        "    targets = targets.view(Batch*Time)\n",
        "\n",
        "    loss = F.cross_entropy(logits, targets) # f is from torch.nn Functional.\n",
        "    return logits, loss\n",
        "\n",
        "    def generate(self,index,max_new_tokens):\n",
        "      for _ in range(max_new_tokens):\n",
        "        #getting the predications\n",
        "        logits, loss = self.forward(indices)\n",
        "        #apply softmax to get probabilities\n",
        "        probs = F.softmax(logits, dim = -1) # this will give us the data in the format btach, Channel\n",
        "        # Sample from the distribution\n",
        "        index_next = torch.multinomial(probs, nun_samples = 1) # )(B,1)\n",
        "        # Append sampled index to the running sequence\n",
        "        indices = torch.cat((indices, index_next), dim = 1) # (B, T+1)\n",
        "      return indices\n",
        "\n"
      ],
      "metadata": {
        "id": "AQdHqIzECVBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "\n",
        "context = torch.zeros((1,1), dtype = torch.long, device = device)\n",
        "generated_chars = decode(m.generate(context, max_new_tokens = 500)[0].tolist())\n",
        "print(generated_chars)"
      ],
      "metadata": {
        "id": "-cWCZ0WZCWnB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}